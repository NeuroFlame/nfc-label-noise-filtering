{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797ec114",
   "metadata": {},
   "source": [
    "### Converting Original Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06bdfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat, savemat\n",
    "from enum import Enum\n",
    "from math import floor\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f480738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772e8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf16_array(array):\n",
    "    \"\"\"Decode MATLAB UTF-16 encoded uint16 arrays to strings.\"\"\"\n",
    "    array = array.flatten() if array.ndim == 2 else array\n",
    "    return ''.join(chr(c) for c in array if c != 0)\n",
    "\n",
    "\n",
    "def handle_cell_string_dataset(dataset, file_handle):\n",
    "    \"\"\"Handle MATLAB-style cell array of strings stored as references.\"\"\"\n",
    "    result = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        ref = dataset[i, 0]\n",
    "        deref = file_handle[ref]\n",
    "        value = deref[()]\n",
    "        if isinstance(value, bytes):\n",
    "            result.append(value.decode('utf-8'))\n",
    "        elif isinstance(value, np.ndarray) and value.dtype == np.uint16:\n",
    "            result.append(decode_utf16_array(value))\n",
    "        else:\n",
    "            result.append(value)\n",
    "    return result\n",
    "\n",
    "def iterate_group(data: h5py.Group, final_data: dict):\n",
    "    \"\"\"\n",
    "    Recursively iterate through a h5py Group and print its structure.\n",
    "    \"\"\"\n",
    "    for key in data.keys():\n",
    "        item = data[key]\n",
    "        if isinstance(item, h5py.Group):\n",
    "            final_data[key] = {}\n",
    "            iterate_group(item, final_data[key])\n",
    "        else:\n",
    "            final_data[key] = item[:]\n",
    "    return final_data\n",
    "\n",
    "def load_data_matfile(path: str, name: list[str]=None):\n",
    "    data = None\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        # List all groups in the file\n",
    "        # print(\"Keys in the file:\", list(f.keys()))\n",
    "        # Access the dataset\n",
    "\n",
    "        if len(name)==0:\n",
    "            data = {}\n",
    "            iterate_group(f, data)\n",
    "            return data\n",
    "        else:\n",
    "            result = {}\n",
    "            for key in name:\n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    # print(\"key is a group, iterating through it\")\n",
    "                    result[key] = iterate_group(f[key], {})\n",
    "                elif key == 'FILE_ID':\n",
    "                    result[key] = handle_cell_string_dataset(f[key], f)\n",
    "                else:\n",
    "                    # print(\"key is a dataset, returning data\")\n",
    "                    data = f[key][:]\n",
    "                    data = data.T  # Transpose to match MATLAB's column-major order\n",
    "                    result[key] = data \n",
    "            return result\n",
    "        \n",
    "def load_result_matfile(path: str):\n",
    "    data = loadmat(path)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8631a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceDataKeys(Enum):\n",
    "    \"\"\"\n",
    "    Enum to represent different keys in the original mat file.\n",
    "    \"\"\"\n",
    "    FILE_ID = 'FILE_ID'\n",
    "    ANALYSIS_ID = 'analysis_ID'\n",
    "    ANALYSIS_SCORE = 'analysis_SCORE'\n",
    "    SFNC = 'sFNC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342f411",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62a7ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fnc_to_features(fnc_path: str, dest_path: str, name: str):\n",
    "    file_path = os.path.join(fnc_path, f'{name}.mat')\n",
    "    original_data = load_data_matfile(\n",
    "        file_path,\n",
    "        name=[\n",
    "            SourceDataKeys.SFNC.value,\n",
    "            SourceDataKeys.FILE_ID.value,\n",
    "            SourceDataKeys.ANALYSIS_SCORE.value,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # --- find diagnosis column (case-insensitive) ---\n",
    "    file_ids = original_data[SourceDataKeys.FILE_ID.value]\n",
    "    label_index = next(\n",
    "        (i for i, col in enumerate(file_ids) if \"diagnosis\" in col.lower()),\n",
    "        None\n",
    "    )\n",
    "    if label_index is None:\n",
    "        raise ValueError(f'No \"diagnosis\" column found in FILE_ID for {name}')\n",
    "\n",
    "    labels = original_data[SourceDataKeys.ANALYSIS_SCORE.value][:, label_index]\n",
    "    # Optional: match MATLAB’s strict check\n",
    "    # uniq = np.unique(labels)\n",
    "    # if not np.all(np.isin(uniq, [1, 2])):\n",
    "    #     raise ValueError(f\"Unexpected diagnosis codes: {uniq.tolist()}\")\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    fnc_matrices = original_data[SourceDataKeys.SFNC.value]  # shape: (N, P, P)\n",
    "    N, P, _ = fnc_matrices.shape\n",
    "\n",
    "    # --- build the lower-triangle mask excluding diagonal (k=-1) ---\n",
    "    mask = np.tril(np.ones((P, P), dtype=bool), k=-1)  # P x P\n",
    "\n",
    "    # --- replicate MATLAB's column-major flattening ---\n",
    "    # 1) reshape each P×P to length P*P along Fortran (column-major) order\n",
    "    # 2) take the linear indices where mask is True, also in Fortran order\n",
    "    linear_idx = np.where(mask.ravel(order='F'))[0]           # size: P*(P-1)/2\n",
    "    fnc_flat_F = fnc_matrices.reshape(N, P * P, order='F')    # N x (P*P)\n",
    "\n",
    "    source_data = fnc_flat_F[:, linear_idx]                   # N x (P*(P-1)/2)\n",
    "    # keep dtype default (float64) to match MATLAB; or cast if you prefer\n",
    "    # source_data = source_data.astype(np.float64, copy=False)\n",
    "\n",
    "    # append labels as the last column\n",
    "    out = np.hstack([source_data, labels])\n",
    "\n",
    "    # save with variable name = dataset name (like MATLAB)\n",
    "    out_path = os.path.join(dest_path, f'{name}.mat')\n",
    "    savemat(out_path, {name: out}, do_compression=True)\n",
    "\n",
    "\n",
    "for i in [\"FBIRN\", \"COBRE\"]:\n",
    "    fnc_dataset = convert_fnc_to_features(\"../original_dataset\", \"dataset\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b97418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "SamplingThs = 0.7\n",
    "iter = 101\n",
    "ntree = 201\n",
    "NI_threshold = 2\n",
    "TypThs = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9e78fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import complete_random_forest.crf as crf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e879f5c",
   "metadata": {},
   "source": [
    "### Running CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing No. 1 CRF for FBIRN dataset\n",
      "Constructing No. 2 CRF for FBIRN dataset\n",
      "Constructing No. 3 CRF for FBIRN dataset\n",
      "Constructing No. 4 CRF for FBIRN dataset\n",
      "Constructing No. 5 CRF for FBIRN dataset\n",
      "Constructing No. 6 CRF for FBIRN dataset\n",
      "Constructing No. 7 CRF for FBIRN dataset\n",
      "Constructing No. 8 CRF for FBIRN dataset\n",
      "Constructing No. 9 CRF for FBIRN dataset\n",
      "Constructing No. 10 CRF for FBIRN dataset\n",
      "Constructing No. 11 CRF for FBIRN dataset\n",
      "Constructing No. 12 CRF for FBIRN dataset\n",
      "Constructing No. 13 CRF for FBIRN dataset\n",
      "Constructing No. 14 CRF for FBIRN dataset\n",
      "Constructing No. 15 CRF for FBIRN dataset\n",
      "Constructing No. 16 CRF for FBIRN dataset\n",
      "Constructing No. 17 CRF for FBIRN dataset\n",
      "Constructing No. 18 CRF for FBIRN dataset\n",
      "Constructing No. 19 CRF for FBIRN dataset\n",
      "Constructing No. 20 CRF for FBIRN dataset\n",
      "Constructing No. 21 CRF for FBIRN dataset\n",
      "Constructing No. 22 CRF for FBIRN dataset\n",
      "Constructing No. 23 CRF for FBIRN dataset\n",
      "Constructing No. 24 CRF for FBIRN dataset\n",
      "Constructing No. 25 CRF for FBIRN dataset\n",
      "Constructing No. 26 CRF for FBIRN dataset\n",
      "Constructing No. 27 CRF for FBIRN dataset\n",
      "Constructing No. 28 CRF for FBIRN dataset\n",
      "Constructing No. 29 CRF for FBIRN dataset\n",
      "Constructing No. 30 CRF for FBIRN dataset\n",
      "Constructing No. 31 CRF for FBIRN dataset\n",
      "Constructing No. 32 CRF for FBIRN dataset\n",
      "Constructing No. 33 CRF for FBIRN dataset\n",
      "Constructing No. 34 CRF for FBIRN dataset\n",
      "Constructing No. 35 CRF for FBIRN dataset\n",
      "Constructing No. 36 CRF for FBIRN dataset\n",
      "Constructing No. 37 CRF for FBIRN dataset\n",
      "Constructing No. 38 CRF for FBIRN dataset\n",
      "Constructing No. 39 CRF for FBIRN dataset\n",
      "Constructing No. 40 CRF for FBIRN dataset\n",
      "Constructing No. 41 CRF for FBIRN dataset\n",
      "Constructing No. 42 CRF for FBIRN dataset\n",
      "Constructing No. 43 CRF for FBIRN dataset\n",
      "Constructing No. 44 CRF for FBIRN dataset\n",
      "Constructing No. 45 CRF for FBIRN dataset\n",
      "Constructing No. 46 CRF for FBIRN dataset\n",
      "Constructing No. 47 CRF for FBIRN dataset\n",
      "Constructing No. 48 CRF for FBIRN dataset\n",
      "Constructing No. 49 CRF for FBIRN dataset\n",
      "Constructing No. 50 CRF for FBIRN dataset\n",
      "Constructing No. 51 CRF for FBIRN dataset\n",
      "Constructing No. 52 CRF for FBIRN dataset\n",
      "Constructing No. 53 CRF for FBIRN dataset\n",
      "Constructing No. 54 CRF for FBIRN dataset\n",
      "Constructing No. 55 CRF for FBIRN dataset\n",
      "Constructing No. 56 CRF for FBIRN dataset\n",
      "Constructing No. 57 CRF for FBIRN dataset\n",
      "Constructing No. 58 CRF for FBIRN dataset\n",
      "Constructing No. 59 CRF for FBIRN dataset\n",
      "Constructing No. 60 CRF for FBIRN dataset\n",
      "Constructing No. 61 CRF for FBIRN dataset\n",
      "Constructing No. 62 CRF for FBIRN dataset\n",
      "Constructing No. 63 CRF for FBIRN dataset\n",
      "Constructing No. 64 CRF for FBIRN dataset\n",
      "Constructing No. 65 CRF for FBIRN dataset\n",
      "Constructing No. 66 CRF for FBIRN dataset\n",
      "Constructing No. 67 CRF for FBIRN dataset\n",
      "Constructing No. 68 CRF for FBIRN dataset\n",
      "Constructing No. 69 CRF for FBIRN dataset\n",
      "Constructing No. 70 CRF for FBIRN dataset\n",
      "Constructing No. 71 CRF for FBIRN dataset\n",
      "Constructing No. 72 CRF for FBIRN dataset\n",
      "Constructing No. 73 CRF for FBIRN dataset\n",
      "Constructing No. 74 CRF for FBIRN dataset\n",
      "Constructing No. 75 CRF for FBIRN dataset\n",
      "Constructing No. 76 CRF for FBIRN dataset\n",
      "Constructing No. 77 CRF for FBIRN dataset\n",
      "Constructing No. 78 CRF for FBIRN dataset\n",
      "Constructing No. 79 CRF for FBIRN dataset\n",
      "Constructing No. 80 CRF for FBIRN dataset\n",
      "Constructing No. 81 CRF for FBIRN dataset\n",
      "Constructing No. 82 CRF for FBIRN dataset\n",
      "Constructing No. 83 CRF for FBIRN dataset\n",
      "Constructing No. 84 CRF for FBIRN dataset\n",
      "Constructing No. 85 CRF for FBIRN dataset\n",
      "Constructing No. 86 CRF for FBIRN dataset\n",
      "Constructing No. 87 CRF for FBIRN dataset\n",
      "Constructing No. 88 CRF for FBIRN dataset\n",
      "Constructing No. 89 CRF for FBIRN dataset\n",
      "Constructing No. 90 CRF for FBIRN dataset\n",
      "Constructing No. 91 CRF for FBIRN dataset\n",
      "Constructing No. 92 CRF for FBIRN dataset\n",
      "Constructing No. 93 CRF for FBIRN dataset\n",
      "Constructing No. 94 CRF for FBIRN dataset\n",
      "Constructing No. 95 CRF for FBIRN dataset\n",
      "Constructing No. 96 CRF for FBIRN dataset\n",
      "Constructing No. 97 CRF for FBIRN dataset\n",
      "Constructing No. 98 CRF for FBIRN dataset\n",
      "Constructing No. 99 CRF for FBIRN dataset\n",
      "Constructing No. 100 CRF for FBIRN dataset\n",
      "Constructing No. 101 CRF for FBIRN dataset\n",
      "Constructing No. 1 CRF for COBRE dataset\n",
      "Constructing No. 2 CRF for COBRE dataset\n",
      "Constructing No. 3 CRF for COBRE dataset\n",
      "Constructing No. 4 CRF for COBRE dataset\n",
      "Constructing No. 5 CRF for COBRE dataset\n",
      "Constructing No. 6 CRF for COBRE dataset\n",
      "Constructing No. 7 CRF for COBRE dataset\n",
      "Constructing No. 8 CRF for COBRE dataset\n",
      "Constructing No. 9 CRF for COBRE dataset\n",
      "Constructing No. 10 CRF for COBRE dataset\n",
      "Constructing No. 11 CRF for COBRE dataset\n",
      "Constructing No. 12 CRF for COBRE dataset\n",
      "Constructing No. 13 CRF for COBRE dataset\n",
      "Constructing No. 14 CRF for COBRE dataset\n",
      "Constructing No. 15 CRF for COBRE dataset\n",
      "Constructing No. 16 CRF for COBRE dataset\n",
      "Constructing No. 17 CRF for COBRE dataset\n",
      "Constructing No. 18 CRF for COBRE dataset\n",
      "Constructing No. 19 CRF for COBRE dataset\n",
      "Constructing No. 20 CRF for COBRE dataset\n",
      "Constructing No. 21 CRF for COBRE dataset\n",
      "Constructing No. 22 CRF for COBRE dataset\n",
      "Constructing No. 23 CRF for COBRE dataset\n",
      "Constructing No. 24 CRF for COBRE dataset\n",
      "Constructing No. 25 CRF for COBRE dataset\n",
      "Constructing No. 26 CRF for COBRE dataset\n",
      "Constructing No. 27 CRF for COBRE dataset\n",
      "Constructing No. 28 CRF for COBRE dataset\n",
      "Constructing No. 29 CRF for COBRE dataset\n",
      "Constructing No. 30 CRF for COBRE dataset\n",
      "Constructing No. 31 CRF for COBRE dataset\n",
      "Constructing No. 32 CRF for COBRE dataset\n",
      "Constructing No. 33 CRF for COBRE dataset\n",
      "Constructing No. 34 CRF for COBRE dataset\n",
      "Constructing No. 35 CRF for COBRE dataset\n",
      "Constructing No. 36 CRF for COBRE dataset\n",
      "Constructing No. 37 CRF for COBRE dataset\n",
      "Constructing No. 38 CRF for COBRE dataset\n",
      "Constructing No. 39 CRF for COBRE dataset\n",
      "Constructing No. 40 CRF for COBRE dataset\n",
      "Constructing No. 41 CRF for COBRE dataset\n",
      "Constructing No. 42 CRF for COBRE dataset\n",
      "Constructing No. 43 CRF for COBRE dataset\n",
      "Constructing No. 44 CRF for COBRE dataset\n",
      "Constructing No. 45 CRF for COBRE dataset\n",
      "Constructing No. 46 CRF for COBRE dataset\n",
      "Constructing No. 47 CRF for COBRE dataset\n",
      "Constructing No. 48 CRF for COBRE dataset\n",
      "Constructing No. 49 CRF for COBRE dataset\n",
      "Constructing No. 50 CRF for COBRE dataset\n",
      "Constructing No. 51 CRF for COBRE dataset\n",
      "Constructing No. 52 CRF for COBRE dataset\n",
      "Constructing No. 53 CRF for COBRE dataset\n",
      "Constructing No. 54 CRF for COBRE dataset\n",
      "Constructing No. 55 CRF for COBRE dataset\n",
      "Constructing No. 56 CRF for COBRE dataset\n",
      "Constructing No. 57 CRF for COBRE dataset\n",
      "Constructing No. 58 CRF for COBRE dataset\n",
      "Constructing No. 59 CRF for COBRE dataset\n",
      "Constructing No. 60 CRF for COBRE dataset\n",
      "Constructing No. 61 CRF for COBRE dataset\n",
      "Constructing No. 62 CRF for COBRE dataset\n",
      "Constructing No. 63 CRF for COBRE dataset\n",
      "Constructing No. 64 CRF for COBRE dataset\n",
      "Constructing No. 65 CRF for COBRE dataset\n",
      "Constructing No. 66 CRF for COBRE dataset\n",
      "Constructing No. 67 CRF for COBRE dataset\n",
      "Constructing No. 68 CRF for COBRE dataset\n",
      "Constructing No. 69 CRF for COBRE dataset\n",
      "Constructing No. 70 CRF for COBRE dataset\n",
      "Constructing No. 71 CRF for COBRE dataset\n",
      "Constructing No. 72 CRF for COBRE dataset\n",
      "Constructing No. 73 CRF for COBRE dataset\n",
      "Constructing No. 74 CRF for COBRE dataset\n",
      "Constructing No. 75 CRF for COBRE dataset\n",
      "Constructing No. 76 CRF for COBRE dataset\n",
      "Constructing No. 77 CRF for COBRE dataset\n",
      "Constructing No. 78 CRF for COBRE dataset\n",
      "Constructing No. 79 CRF for COBRE dataset\n",
      "Constructing No. 80 CRF for COBRE dataset\n",
      "Constructing No. 81 CRF for COBRE dataset\n",
      "Constructing No. 82 CRF for COBRE dataset\n",
      "Constructing No. 83 CRF for COBRE dataset\n",
      "Constructing No. 84 CRF for COBRE dataset\n",
      "Constructing No. 85 CRF for COBRE dataset\n",
      "Constructing No. 86 CRF for COBRE dataset\n",
      "Constructing No. 87 CRF for COBRE dataset\n",
      "Constructing No. 88 CRF for COBRE dataset\n",
      "Constructing No. 89 CRF for COBRE dataset\n",
      "Constructing No. 90 CRF for COBRE dataset\n",
      "Constructing No. 91 CRF for COBRE dataset\n",
      "Constructing No. 92 CRF for COBRE dataset\n",
      "Constructing No. 93 CRF for COBRE dataset\n",
      "Constructing No. 94 CRF for COBRE dataset\n",
      "Constructing No. 95 CRF for COBRE dataset\n",
      "Constructing No. 96 CRF for COBRE dataset\n",
      "Constructing No. 97 CRF for COBRE dataset\n",
      "Constructing No. 98 CRF for COBRE dataset\n",
      "Constructing No. 99 CRF for COBRE dataset\n",
      "Constructing No. 100 CRF for COBRE dataset\n",
      "Constructing No. 101 CRF for COBRE dataset\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "def perform_crf(\n",
    "    dataset_path: str,\n",
    "    result_path: str,\n",
    "    name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    consisting of subjects with there features as columns\n",
    "    last column is label of the subject\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(dataset_path, f'{name}.mat')\n",
    "    dataset = load_result_matfile(file_path)[name]\n",
    "\n",
    "    subject_count, features_count = dataset.shape\n",
    "\n",
    "    labels = dataset[:, -1]\n",
    "    label_classes = np.unique(labels)\n",
    "    len_label_classes = len(label_classes)\n",
    "    # class is either 1 (SZ) or 2 (HC)\n",
    "    original_cls_label_indexes = dict()\n",
    "    sampling_cls_label_count = [None] * len_label_classes\n",
    "\n",
    "    for i in range(len_label_classes):\n",
    "        original_cls_label_indexes[label_classes[i]] = np.where(\n",
    "            labels == label_classes[i]\n",
    "        )[0]\n",
    "        sampling_cls_label_count[i] = floor(\n",
    "            len(original_cls_label_indexes[label_classes[i]]) * SamplingThs\n",
    "        )\n",
    "\n",
    "    dtype = np.dtype([\n",
    "        (\"subject_id\", np.int64),   # col 0\n",
    "        (\"count\", np.int64),        # col 1\n",
    "        (\"non_noise_count\", np.int64),      # col 2\n",
    "        (\"ratio\", np.float64)       # col 3\n",
    "    ])\n",
    "\n",
    "    sub_noise_per_iter = np.zeros(subject_count, dtype=dtype)\n",
    "    sub_noise_per_iter[\"subject_id\"] = np.arange(1, subject_count + 1, dtype=int) # id's of all subjects\n",
    "    mean_sub_sampling_length = int(np.mean(sampling_cls_label_count))\n",
    "\n",
    "    non_noise_sampling_subjects = [] # for each sampling, subjects Ids which are not noise\n",
    "    nltc_decisions = []\n",
    "    for i in range(iter):\n",
    "        print(f\"Constructing No. {i+1} CRF for {name} dataset\")\n",
    "        index_temp = []\n",
    "        for cls in label_classes:\n",
    "            random_sampling_indexes = np.random.permutation(\n",
    "                original_cls_label_indexes[cls]\n",
    "            )[:mean_sub_sampling_length]\n",
    "            index_temp.extend(random_sampling_indexes)\n",
    "\n",
    "        random_indices = np.array(index_temp).astype(int)\n",
    "\n",
    "        sub_noise_per_iter['count'][random_indices] += 1\n",
    "        sampled_dataset = dataset[random_indices, :]\n",
    "        attributes = zscore(sampled_dataset[:, 0:features_count-1], axis=0, ddof=1)\n",
    "        sampled_dataset_labels = sampled_dataset[:, -1].reshape(-1,1)\n",
    "\n",
    "        training_data = np.hstack((sampled_dataset_labels, attributes))\n",
    "\n",
    "        # denoise_data, non_noise_ID, NLTC_labels =  running_crf(training_data, ntree, NI_threshold)\n",
    "        instance = crf.CRF(ntree, NI_threshold)\n",
    "        non_noise_ids, nltc_labels = instance.crf_v1(training_data, ntree)\n",
    "        nltc_decisions.append(nltc_labels)\n",
    "        # print(\"picked sampling number: \", sampling_indexes[non_noise_ids, i])\n",
    "        non_noise_sampling_subjects.append(random_indices[non_noise_ids])\n",
    "\n",
    "    denoise_check = np.zeros((subject_count,), dtype=int)\n",
    "    for i in range(iter):\n",
    "        idxs = non_noise_sampling_subjects[i]\n",
    "        denoise_check [idxs] += 1\n",
    "\n",
    "    # print(\"denoise count for each subject: \", denoise_check)\n",
    "    sub_noise_per_iter['non_noise_count'] = denoise_check\n",
    "\n",
    "    np.divide(sub_noise_per_iter['non_noise_count'], sub_noise_per_iter['count'], out=sub_noise_per_iter['ratio'], where=sub_noise_per_iter['count'] != 0)\n",
    "    sub_noise_per_iter['ratio'] = np.round(sub_noise_per_iter['ratio'], 1)\n",
    "\n",
    "    final_mat = np.column_stack((sub_noise_per_iter[\"subject_id\"], sub_noise_per_iter[\"count\"], sub_noise_per_iter[\"non_noise_count\"], sub_noise_per_iter['ratio']))\n",
    "    # print(\"iteration matrix: \", final_mat)\n",
    "\n",
    "    output_path = os.path.join(result_path, f'{name}_CRF.mat')\n",
    "    savemat(output_path, {\n",
    "        'count': final_mat,\n",
    "        'nltc_labels': nltc_decisions,\n",
    "        'non_noise_ind': non_noise_sampling_subjects\n",
    "    }, do_compression=True)\n",
    "\n",
    "\n",
    "for name in [\"FBIRN\", 'COBRE']:\n",
    "    perform_crf('dataset', 'results', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aafce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  79  78 1.0]\n",
      "[  2  83  83 1.0]\n",
      "[  3  80  74 0.9]\n",
      "[  4  58  58 1.0]\n",
      "[  5  60  60 1.0]\n",
      "[  6  78  76 1.0]\n",
      "[  7  64  64 1.0]\n",
      "[  8  55  55 1.0]\n",
      "[  9  86  86 1.0]\n",
      "[ 10  69  69 1.0]\n",
      "[ 11  82  82 1.0]\n",
      "[ 12  51  50 1.0]\n",
      "[ 13  81   5 0.1]\n",
      "[ 14  61  52 0.9]\n",
      "[ 15  69  69 1.0]\n",
      "[ 16  60   4 0.1]\n",
      "[ 17  83   0 0.0]\n",
      "[ 18  80  75 0.9]\n",
      "[ 19  69  12 0.2]\n",
      "[ 20  60  60 1.0]\n",
      "[ 21  58  58 1.0]\n",
      "[ 22  54  50 0.9]\n",
      "[ 23  60  10 0.2]\n",
      "[ 24  85   1 0.0]\n",
      "[ 25  61  61 1.0]\n",
      "[ 26  66  66 1.0]\n",
      "[ 27  92  92 1.0]\n",
      "[ 28  82  77 0.9]\n",
      "[ 29  85  85 1.0]\n",
      "[ 30  65  31 0.5]\n",
      "[ 31  83  83 1.0]\n",
      "[ 32  77   7 0.1]\n",
      "[ 33  49   4 0.1]\n",
      "[ 34  81  80 1.0]\n",
      "[ 35  68  68 1.0]\n",
      "[ 36  55  54 1.0]\n",
      "[ 37  65  65 1.0]\n",
      "[ 38  63   9 0.1]\n",
      "[ 39  64  64 1.0]\n",
      "[ 40  74  74 1.0]\n",
      "[ 41  70  70 1.0]\n",
      "[ 42  77   6 0.1]\n",
      "[ 43  48  48 1.0]\n",
      "[ 44  64   0 0.0]\n",
      "[ 45  82  77 0.9]\n",
      "[ 46  63  63 1.0]\n",
      "[ 47  66  66 1.0]\n",
      "[ 48  74  19 0.3]\n",
      "[ 49  63   0 0.0]\n",
      "[ 50  70  70 1.0]\n",
      "[ 51  81  59 0.7]\n",
      "[ 52  62  62 1.0]\n",
      "[ 53  78  77 1.0]\n",
      "[ 54  73   2 0.0]\n",
      "[ 55  81  80 1.0]\n",
      "[ 56  78  78 1.0]\n",
      "[ 57  82  79 1.0]\n",
      "[ 58  60  13 0.2]\n",
      "[ 59  83  62 0.7]\n",
      "[ 60  75  75 1.0]\n",
      "[ 61  62  62 1.0]\n",
      "[ 62  75  75 1.0]\n",
      "[ 63  64  56 0.9]\n",
      "[ 64  76  64 0.8]\n",
      "[ 65  82  82 1.0]\n",
      "[ 66  57   7 0.1]\n",
      "[ 67  63  63 1.0]\n",
      "[ 68  71  71 1.0]\n",
      "[ 69  64  11 0.2]\n",
      "[ 70  61   1 0.0]\n",
      "[ 71  66  65 1.0]\n",
      "[ 72  66  35 0.5]\n",
      "[ 73  81  68 0.8]\n",
      "[ 74  82  82 1.0]\n",
      "[ 75  54   4 0.1]\n",
      "[ 76  83  83 1.0]\n",
      "[ 77  78  71 0.9]\n",
      "[ 78  90  86 1.0]\n",
      "[ 79  86  86 1.0]\n",
      "[ 80  83  56 0.7]\n",
      "[ 81  70  70 1.0]\n",
      "[ 82  81  76 0.9]\n",
      "[ 83  83  83 1.0]\n",
      "[ 84  69  30 0.4]\n",
      "[ 85  60  60 1.0]\n",
      "[ 86  83  82 1.0]\n",
      "[ 87  57   7 0.1]\n",
      "[ 88  58  58 1.0]\n",
      "[ 89  64  17 0.3]\n",
      "[ 90  66  61 0.9]\n",
      "[ 91  56  34 0.6]\n",
      "[ 92  64  49 0.8]\n",
      "[ 93  56   2 0.0]\n",
      "[ 94  66  65 1.0]\n",
      "[ 95  62   6 0.1]\n",
      "[ 96  57  45 0.8]\n",
      "[ 97  58  12 0.2]\n",
      "[ 98  81  77 1.0]\n",
      "[ 99  51  35 0.7]\n",
      "[100  77  76 1.0]\n",
      "[101  84  79 0.9]\n",
      "[102  72  71 1.0]\n",
      "[103  67  67 1.0]\n",
      "[104  57  57 1.0]\n",
      "[105  88  88 1.0]\n",
      "[106  83  52 0.6]\n",
      "[107  78  78 1.0]\n",
      "[108  80  78 1.0]\n",
      "[109  60  12 0.2]\n",
      "[110  71  67 0.9]\n",
      "[111  68  61 0.9]\n",
      "[112  78  77 1.0]\n",
      "[113  77  77 1.0]\n",
      "[114  59  59 1.0]\n",
      "[115  61  61 1.0]\n",
      "[116  74  67 0.9]\n",
      "[117  86  86 1.0]\n",
      "[118  58   8 0.1]\n",
      "[119  55   1 0.0]\n",
      "[120  53  53 1.0]\n",
      "[121  66  66 1.0]\n",
      "[122  59  57 1.0]\n",
      "[123  52  35 0.7]\n",
      "[124  64   0 0.0]\n",
      "[125  73   7 0.1]\n",
      "[126  66  55 0.8]\n",
      "[127  65  38 0.6]\n",
      "[128  54  50 0.9]\n",
      "[129  60  60 1.0]\n",
      "[130  59  59 1.0]\n",
      "[131  59   5 0.1]\n",
      "[132  57  24 0.4]\n",
      "[133  67  67 1.0]\n",
      "[134  66  51 0.8]\n",
      "[135  82  17 0.2]\n",
      "[136  75  73 1.0]\n",
      "[137  59   0 0.0]\n",
      "[138  59  42 0.7]\n",
      "[139  82  80 1.0]\n",
      "[140  83  78 0.9]\n",
      "[141  76  66 0.9]\n",
      "[142  56   3 0.1]\n",
      "[143  61  24 0.4]\n",
      "[144  65   0 0.0]\n",
      "[145  84  75 0.9]\n",
      "[146  56   0 0.0]\n",
      "[147  68  68 1.0]\n",
      "[148  77  77 1.0]\n",
      "[149  63  16 0.3]\n",
      "[150  83  83 1.0]\n",
      "[151  62  62 1.0]\n",
      "[152  84  80 1.0]\n",
      "[153  73  44 0.6]\n",
      "[154  63   5 0.1]\n",
      "[155  84  76 0.9]\n",
      "[156  83  83 1.0]\n",
      "[157  66  66 1.0]\n",
      "[  1  79  78 1.0]\n",
      "[  2  83  83 1.0]\n",
      "[  3  80  74 0.9]\n",
      "[  4  58  58 1.0]\n",
      "[  5  60  60 1.0]\n",
      "[  6  78  76 1.0]\n",
      "[  7  64  64 1.0]\n",
      "[  8  55  55 1.0]\n",
      "[  9  86  86 1.0]\n",
      "[ 10  69  69 1.0]\n",
      "[ 11  82  82 1.0]\n",
      "[ 12  51  50 1.0]\n",
      "[ 13  81   5 0.1]\n",
      "[ 14  61  52 0.9]\n",
      "[ 15  69  69 1.0]\n",
      "[ 16  60   4 0.1]\n",
      "[ 17  83   0 0.0]\n",
      "[ 18  80  75 0.9]\n",
      "[ 19  69  12 0.2]\n",
      "[ 20  60  60 1.0]\n",
      "[ 21  58  58 1.0]\n",
      "[ 22  54  50 0.9]\n",
      "[ 23  60  10 0.2]\n",
      "[ 24  85   1 0.0]\n",
      "[ 25  61  61 1.0]\n",
      "[ 26  66  66 1.0]\n",
      "[ 27  92  92 1.0]\n",
      "[ 28  82  77 0.9]\n",
      "[ 29  85  85 1.0]\n",
      "[ 30  65  31 0.5]\n",
      "[ 31  83  83 1.0]\n",
      "[ 32  77   7 0.1]\n",
      "[ 33  49   4 0.1]\n",
      "[ 34  81  80 1.0]\n",
      "[ 35  68  68 1.0]\n",
      "[ 36  55  54 1.0]\n",
      "[ 37  65  65 1.0]\n",
      "[ 38  63   9 0.1]\n",
      "[ 39  64  64 1.0]\n",
      "[ 40  74  74 1.0]\n",
      "[ 41  70  70 1.0]\n",
      "[ 42  77   6 0.1]\n",
      "[ 43  48  48 1.0]\n",
      "[ 44  64   0 0.0]\n",
      "[ 45  82  77 0.9]\n",
      "[ 46  63  63 1.0]\n",
      "[ 47  66  66 1.0]\n",
      "[ 48  74  19 0.3]\n",
      "[ 49  63   0 0.0]\n",
      "[ 50  70  70 1.0]\n",
      "[ 51  81  59 0.7]\n",
      "[ 52  62  62 1.0]\n",
      "[ 53  78  77 1.0]\n",
      "[ 54  73   2 0.0]\n",
      "[ 55  81  80 1.0]\n",
      "[ 56  78  78 1.0]\n",
      "[ 57  82  79 1.0]\n",
      "[ 58  60  13 0.2]\n",
      "[ 59  83  62 0.7]\n",
      "[ 60  75  75 1.0]\n",
      "[ 61  62  62 1.0]\n",
      "[ 62  75  75 1.0]\n",
      "[ 63  64  56 0.9]\n",
      "[ 64  76  64 0.8]\n",
      "[ 65  82  82 1.0]\n",
      "[ 66  57   7 0.1]\n",
      "[ 67  63  63 1.0]\n",
      "[ 68  71  71 1.0]\n",
      "[ 69  64  11 0.2]\n",
      "[ 70  61   1 0.0]\n",
      "[ 71  66  65 1.0]\n",
      "[ 72  66  35 0.5]\n",
      "[ 73  81  68 0.8]\n",
      "[ 74  82  82 1.0]\n",
      "[ 75  54   4 0.1]\n",
      "[ 76  83  83 1.0]\n",
      "[ 77  78  71 0.9]\n",
      "[ 78  90  86 1.0]\n",
      "[ 79  86  86 1.0]\n",
      "[ 80  83  56 0.7]\n",
      "[ 81  70  70 1.0]\n",
      "[ 82  81  76 0.9]\n",
      "[ 83  83  83 1.0]\n",
      "[ 84  69  30 0.4]\n",
      "[ 85  60  60 1.0]\n",
      "[ 86  83  82 1.0]\n",
      "[ 87  57   7 0.1]\n",
      "[ 88  58  58 1.0]\n",
      "[ 89  64  17 0.3]\n",
      "[ 90  66  61 0.9]\n",
      "[ 91  56  34 0.6]\n",
      "[ 92  64  49 0.8]\n",
      "[ 93  56   2 0.0]\n",
      "[ 94  66  65 1.0]\n",
      "[ 95  62   6 0.1]\n",
      "[ 96  57  45 0.8]\n",
      "[ 97  58  12 0.2]\n",
      "[ 98  81  77 1.0]\n",
      "[ 99  51  35 0.7]\n",
      "[100  77  76 1.0]\n",
      "[101  84  79 0.9]\n",
      "[102  72  71 1.0]\n",
      "[103  67  67 1.0]\n",
      "[104  57  57 1.0]\n",
      "[105  88  88 1.0]\n",
      "[106  83  52 0.6]\n",
      "[107  78  78 1.0]\n",
      "[108  80  78 1.0]\n",
      "[109  60  12 0.2]\n",
      "[110  71  67 0.9]\n",
      "[111  68  61 0.9]\n",
      "[112  78  77 1.0]\n",
      "[113  77  77 1.0]\n",
      "[114  59  59 1.0]\n",
      "[115  61  61 1.0]\n",
      "[116  74  67 0.9]\n",
      "[117  86  86 1.0]\n",
      "[118  58   8 0.1]\n",
      "[119  55   1 0.0]\n",
      "[120  53  53 1.0]\n",
      "[121  66  66 1.0]\n",
      "[122  59  57 1.0]\n",
      "[123  52  35 0.7]\n",
      "[124  64   0 0.0]\n",
      "[125  73   7 0.1]\n",
      "[126  66  55 0.8]\n",
      "[127  65  38 0.6]\n",
      "[128  54  50 0.9]\n",
      "[129  60  60 1.0]\n",
      "[130  59  59 1.0]\n",
      "[131  59   5 0.1]\n",
      "[132  57  24 0.4]\n",
      "[133  67  67 1.0]\n",
      "[134  66  51 0.8]\n",
      "[135  82  17 0.2]\n",
      "[136  75  73 1.0]\n",
      "[137  59   0 0.0]\n",
      "[138  59  42 0.7]\n",
      "[139  82  80 1.0]\n",
      "[140  83  78 0.9]\n",
      "[141  76  66 0.9]\n",
      "[142  56   3 0.1]\n",
      "[143  61  24 0.4]\n",
      "[144  65   0 0.0]\n",
      "[145  84  75 0.9]\n",
      "[146  56   0 0.0]\n",
      "[147  68  68 1.0]\n",
      "[148  77  77 1.0]\n",
      "[149  63  16 0.3]\n",
      "[150  83  83 1.0]\n",
      "[151  62  62 1.0]\n",
      "[152  84  80 1.0]\n",
      "[153  73  44 0.6]\n",
      "[154  63   5 0.1]\n",
      "[155  84  76 0.9]\n",
      "[156  83  83 1.0]\n",
      "[157  66  66 1.0]\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(mat):\n",
    "    \"\"\"Print with ints for first 3 cols, 1 decimal for last.\"\"\"\n",
    "    ids     = mat[:, 0].astype(np.int64)\n",
    "    count   = mat[:, 1].astype(np.int64)\n",
    "    success = mat[:, 2].astype(np.int64)\n",
    "    ratio   = mat[:, 3]\n",
    "    rows = [f\"[{i:>3d} {c:>3d} {s:>3d} {r:>3.1f}]\" for i,c,s,r in zip(ids, count, success, ratio)]\n",
    "    print(\"\\n\".join(rows))\n",
    "\n",
    "for name in [\"FBIRN\", \"COBRE\"]:\n",
    "    crf = load_result_matfile('results/COBRE_CRF.mat')['count']\n",
    "    pretty_print(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6975fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_typical_subjects(input_path: str, data_path: str, name: str, typical_threshold: float) -> None:\n",
    "\n",
    "    input_file = os.path.join(input_path, f'{name}_CRF.mat')\n",
    "    crf_count = load_result_matfile(input_file)['count']\n",
    "    crf_count[:, 3] = np.round(crf_count[:, 3], 1)\n",
    "\n",
    "    data_file = os.path.join(data_path, f'{name}.mat')\n",
    "    data = load_result_matfile(data_file)[name]\n",
    "\n",
    "    original_labels = data[:, -1]\n",
    "    \n",
    "    mask_sz = (crf_count[:,3] >= typical_threshold) & (original_labels == 1)\n",
    "    mask_hc = (crf_count[:,3] >= typical_threshold) & (original_labels == 2)\n",
    "\n",
    "    typical_group_sz = np.where(mask_sz)[0]\n",
    "    typical_group_hc = np.where(mask_hc)[0]\n",
    "\n",
    "    savemat(f'results/{name}_Typ.mat', {\n",
    "        'typical_sz': typical_group_sz,\n",
    "        'typical_hc': typical_group_hc,\n",
    "    }, do_compression=True)\n",
    "\n",
    "for name in [\"FBIRN\", \"COBRE\"]:\n",
    "    find_typical_subjects('results', 'dataset', name, TypThs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a2287",
   "metadata": {},
   "source": [
    "### Finding Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1919f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# cummulative feature selection using Bonferroni corrected threshold 0.01/(Col-1)\n",
    "def cumulative_features_selection(Pval, PvalPara):\n",
    "    # Pval: 1-D array of p-values\n",
    "    FeaInd = np.argsort(Pval)                  # indices sorted by p-value\n",
    "    SortPval = Pval[FeaInd]                    # sorted p-values\n",
    "\n",
    "    # Find first index where p-value exceeds threshold\n",
    "    above_thresh = np.where(SortPval > PvalPara)[0]\n",
    "    if above_thresh.size > 0:\n",
    "        ind = above_thresh[0]                  # first index above threshold\n",
    "    else:\n",
    "        ind = len(SortPval)                    # all p-values below threshold\n",
    "\n",
    "    Fea = FeaInd[:ind]                         # keep only significant features\n",
    "    return Fea\n",
    "\n",
    "def compute_score(independent_data, typical_data):\n",
    "    col = independent_data.shape[1]\n",
    "    \n",
    "    ind_fea = independent_data[:, :-1]\n",
    "    typical_data_features = typical_data[:, : -1]\n",
    "    \n",
    "    typical_data_labels = typical_data[:, -1]\n",
    "    typical_group_sz = typical_data_features[typical_data_labels == 1, :]\n",
    "    typical_group_hc = typical_data_features[typical_data_labels == 2, :]\n",
    "    \n",
    "    t_stat, p_val = stats.ttest_ind(typical_group_sz, typical_group_hc, axis=0, equal_var=True)\n",
    "    \n",
    "    # print('pvals: ', p_val)\n",
    "    \n",
    "    significant_threshold = 0.01 / (col - 1)\n",
    "    \n",
    "    selected_features = cumulative_features_selection(p_val, significant_threshold)\n",
    "    # print('selected features: ', selected_features)\n",
    "    \n",
    "    center_sz = np.mean(typical_group_sz[:, selected_features], axis=0).reshape(1, -1)\n",
    "    center_hc = np.mean(typical_group_hc[:, selected_features], axis=0).reshape(1, -1)\n",
    "    \n",
    "    \n",
    "    # print(center_sz, center_hc)\n",
    "    \n",
    "    X = ind_fea[:, selected_features]\n",
    "    dist1 = cdist(X, center_sz)\n",
    "    dist2 = cdist(X, center_hc)\n",
    "    \n",
    "    distance_typical_group_sz = dist1.mean(axis=1)\n",
    "    distance_typical_group_hc = dist2.mean(axis=1)\n",
    "    \n",
    "    total_distance = distance_typical_group_sz + distance_typical_group_hc\n",
    "    \n",
    "    A = distance_typical_group_sz / total_distance\n",
    "    B = distance_typical_group_hc / total_distance\n",
    "    \n",
    "    scores = np.tan((A-B)*np.pi / 2)\n",
    "    # print('final_scores: ', scores)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "\n",
    "def predict_score(subjects: list[str], input_path: str, dataset_path: str, typical_threshold: float):\n",
    "    \n",
    "    total_columns = len(subjects)+1\n",
    "    all_cols = np.arange(total_columns)\n",
    "    \n",
    "    for subject_id in range(len(subjects)):\n",
    "        \n",
    "        sub_name = subjects[subject_id]\n",
    "        \n",
    "        data_file = os.path.join(dataset_path, f'{sub_name}.mat')\n",
    "        independent_data = load_result_matfile(data_file)[sub_name]\n",
    "        subject_count, _ = independent_data.shape\n",
    "        \n",
    "        ind_sub_labels = independent_data[:, -1]\n",
    "        total_columns = len(subjects)+1\n",
    "        independent_score = np.zeros((subject_count, total_columns))\n",
    "        \n",
    "        for main_sub_id in range(len(subjects)):\n",
    "            if main_sub_id == subject_id:\n",
    "                independent_score[:, subject_id] = ind_sub_labels\n",
    "                continue\n",
    "            \n",
    "            main_sub_name = subjects[main_sub_id]\n",
    "            main_sub_path = os.path.join(dataset_path, f'{main_sub_name}.mat')\n",
    "            main_sub_data = load_result_matfile(main_sub_path)[main_sub_name]\n",
    "            \n",
    "            input_crf_path = os.path.join(input_path, f'{main_sub_name}_CRF.mat')\n",
    "            main_sub_crf = load_result_matfile(input_crf_path)['count']\n",
    "            \n",
    "            typical_data = main_sub_data[main_sub_crf[:, 3] >= typical_threshold, :]\n",
    "\n",
    "            independent_score[:, main_sub_id] = compute_score(independent_data, typical_data)\n",
    "        independent_score[:, -1] = independent_score[:, np.setdiff1d(all_cols, [subject_id, total_columns-1])].mean(axis=1)\n",
    "        idx = (independent_score[:, :len(subjects)] == 0).any(axis=1)\n",
    "        independent_score[idx, -1] = 0\n",
    "        \n",
    "        output_path = os.path.join(input_path, f'{sub_name}_Score.mat')\n",
    "        savemat(output_path, {\n",
    "            sub_name: independent_score\n",
    "        }, do_compression = True)\n",
    "    \n",
    "predict_score(['FBIRN', 'COBRE'], 'results/', 'dataset/', TypThs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1431f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: FBIRN Group: SZ\n",
      "shape: (1, 100) and type: <class 'numpy.ndarray'>\n",
      "Total no.of Typical Subjects:  1\n",
      "[[  0   1   3   5   7  18  20  23  24  27  39  41  43  44  46  48  51  56\n",
      "   59  61  62  63  68  69  72  73  77  78  79  80  86  92  93  94  98  99\n",
      "  101 107 113 119 120 121 125 127 132 138 139 142 147 153 154 159 161 166\n",
      "  167 169 170 181 185 188 193 194 197 201 202 203 204 205 209 220 224 228\n",
      "  229 231 233 234 240 241 243 246 250 252 255 259 260 267 272 275 279 281\n",
      "  283 286 289 290 297 298 299 302 303 305]]\n",
      "Dataset: FBIRN Group: HC\n",
      "shape: (1, 95) and type: <class 'numpy.ndarray'>\n",
      "Total no.of Typical Subjects:  1\n",
      "[[  6   9  10  11  14  15  19  22  25  26  28  33  36  38  42  45  47  49\n",
      "   52  55  57  58  71  74  75  81  84  85  87  95  97 100 105 106 110 117\n",
      "  118 122 123 135 140 141 144 148 150 156 160 162 163 164 171 172 179 183\n",
      "  187 190 191 206 208 211 212 216 218 219 222 225 232 235 236 237 238 239\n",
      "  242 244 245 249 257 261 262 263 265 273 274 276 277 280 284 285 287 293\n",
      "  294 295 304 309 310]]\n",
      "\n",
      "\n",
      "\n",
      "Dataset: COBRE Group: SZ\n",
      "shape: (1, 55) and type: <class 'numpy.ndarray'>\n",
      "Total no.of Typical Subjects:  1\n",
      "[[  0   1   2   5   8  10  17  26  27  28  30  33  39  44  52  54  55  56\n",
      "   59  61  63  64  67  72  73  75  76  77  78  80  81  82  85  97  99 100\n",
      "  101 104 106 107 109 111 112 115 116 135 138 139 140 144 147 149 151 154\n",
      "  155]]\n",
      "Dataset: COBRE Group: HC\n",
      "shape: (1, 50) and type: <class 'numpy.ndarray'>\n",
      "Total no.of Typical Subjects:  1\n",
      "[[  3   4   6   7   9  11  13  14  19  20  21  24  25  34  35  36  38  40\n",
      "   42  45  46  49  51  60  62  66  70  84  87  89  91  93  95 102 103 110\n",
      "  113 114 119 120 121 125 127 128 129 132 133 146 150 156]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataName = [\"FBIRN\", 'COBRE']\n",
    "\n",
    "def handle_typical_groups(name: str, group: str, data: h5py.Dataset):\n",
    "    print(f\"Dataset: {i}\", f\"Group: {group}\")\n",
    "    \n",
    "    # single row, consisting of subject ID's \n",
    "    # representing typical subjects after all the iteration \n",
    "    print(f\"shape: {data.shape} and type: {type(data)}\")\n",
    "    print(\"Total no.of Typical Subjects: \", data.shape[0])\n",
    "    print(data)\n",
    "\n",
    "for i in DataName:\n",
    "    path= f'results/{i}_Typ.mat'\n",
    "    data = load_result_matfile(path)\n",
    "    \n",
    "    handle_typical_groups(i, \"SZ\", data['typical_sz'])\n",
    "    handle_typical_groups(i, \"HC\", data['typical_hc'])\n",
    "    \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c86761cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Independent Score: FBIRN ========\n",
      "Shape: (311, 3) and type: <class 'numpy.ndarray'>\n",
      "top 5 subjects and there score comparing other dataset\n",
      "[[ 1.         -0.30615022 -0.30615022]\n",
      " [ 1.         -0.48583681 -0.48583681]\n",
      " [ 1.         -0.08907705 -0.08907705]\n",
      " [ 1.         -0.40001881 -0.40001881]\n",
      " [ 2.         -0.43887241 -0.43887241]]\n",
      "count of subjects having SZ:  164\n",
      "count of subjects who are healthy:  147\n",
      "======= Independent Score: COBRE ========\n",
      "Shape: (157, 3) and type: <class 'numpy.ndarray'>\n",
      "top 5 subjects and there score comparing other dataset\n",
      "[[-0.2166395   1.         -0.2166395 ]\n",
      " [-0.46306541  1.         -0.46306541]\n",
      " [-0.23880238  1.         -0.23880238]\n",
      " [ 0.05152334  2.          0.05152334]\n",
      " [ 0.55858622  2.          0.55858622]]\n",
      "count of subjects having SZ:  41\n",
      "count of subjects who are healthy:  116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def handle_Indep_Score(name: str, data: h5py.Dataset):\n",
    "    print(f\"======= Independent Score: {name} ========\")\n",
    "    \n",
    "    # total subjects of current dataset name, then its classification comparing other dataset \n",
    "    print(f\"Shape: {data.shape} and type: {type(data)}\")\n",
    "    print(\"top 5 subjects and there score comparing other dataset\")\n",
    "    print(data[:5][:])\n",
    "    \n",
    "    total_subjects = data.shape[0]\n",
    "    avg_column = data[:, -1]\n",
    "    sz_subjects_count = np.count_nonzero(avg_column >= 0)\n",
    "    hc_subjects_count = np.count_nonzero(avg_column < 0)\n",
    "    print(\"count of subjects having SZ: \", sz_subjects_count)\n",
    "    print(\"count of subjects who are healthy: \", hc_subjects_count)\n",
    "    \n",
    "\n",
    "DataName = ['FBIRN', 'COBRE']\n",
    "for i in DataName:\n",
    "    path = f'results/{i}_Score.mat'\n",
    "    data = load_result_matfile(path)[i]\n",
    "    handle_Indep_Score(i, data)\n",
    "    # print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e9cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  a  b  c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "my_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "\n",
    "df = pd.DataFrame([my_dict.keys()])\n",
    "print(df)\n",
    "# print(df['a'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
